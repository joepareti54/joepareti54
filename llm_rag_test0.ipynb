{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDZHtvZsA/GoJZ2N/hAAcA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joepareti54/joepareti54/blob/main/llm_rag_test0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-xK1DQrPSkK",
        "outputId": "c8859b58-e170-4152-88ca-b8ecb8868677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.25.1)\n",
            "Applying natural language \n",
            "processing to cancer \n",
            "genomics\n",
            "Lena Pfitzer\n",
            "Principal Data Scientist\n",
            "myNEO\n",
            "Presented by\n",
            "2\n",
            "What are neoantigens?\n",
            "What is immunotherapy?\n",
            "How does myNEO apply NLP to cancer genomics?\n",
            "Outline\n",
            "3\n",
            "What are neoantigens?\n",
            "4\n",
            "What is cancer?\n",
            "Gene\n",
            "Protein\n",
            "Mutations\n",
            "A cancer cell arises when accumulated mutations are left unrepaired…\n",
            "What is cancer?\n",
            "5\n",
            "… causing uncontrolled growth and cell proliferation\n",
            "Mutations\n",
            "A cancer cell arises when accumulated mutations are left unrepaired…\n",
            "What is cancer?\n",
            "6\n",
            "ACTCGTTAGGCA\n",
            "ACTCCTTAGGCA\n",
            "Mutated protein\n",
            "Peptides\n",
            "Neoantigens\n",
            "Neoantigens are presented on the cancer \n",
            "cell surface\n",
            "8\n",
            "The immune system can recognise these \n",
            "neoantigens and attacks the tumour cells\n",
            "9\n",
            "10\n",
            "What is immunotherapy?\n",
            "Immunotherapy stimulates the immune system to recognise \n",
            "these molecules and thus attack all tumour cells\n",
            "11\n",
            "Types of immunotherapy\n",
            "Adoptive Cell Transfer\n",
            "Cancer Vaccine\n",
            "Oncolytic Viruses\n",
            "Targeted Antibodies\n",
            "Immunotherapy uses the power of the body’s own immune system to prevent, control, and eliminate cancer.\n",
            "Immune Checkpoint Inhibitors\n",
            "Types of immunotherapy\n",
            "Adoptive Cell Transfer\n",
            "Cancer Vaccine\n",
            "Oncolytic Viruses\n",
            "Immune Checkpoint Inhibitors\n",
            "Targeted Antibodies\n",
            "Immunotherapy uses the power of the body’s own immune system to prevent, control, and eliminate cancer.\n",
            "PD-1\n",
            "PD-L1\n",
            "ICI\n",
            "Immune Checkpoint Inhibitors (ICI)\n",
            "Checkpoint inhibitors work by blocking immune \n",
            "checkpoints that tumours frequently manipulate \n",
            "in order to shut down immune responses and \n",
            "protect themselves. \n",
            "14\n",
            "Generic immunotherapy (e.g. checkpoint inhibitor therapy) has shown unprecedented results\n",
            "1st line advanced\n",
            "cancers\n",
            "Pembrolizumab\n",
            "(anti-PD1) is first \n",
            "line treatment in \n",
            "metastatic NSCLC\n",
            "4x more \n",
            "effective\n",
            "Can increase\n",
            "survival rates up to\n",
            "40%\n",
            "Fewer side \n",
            "effects\n",
            "No administration\n",
            "of toxic compounds\n",
            "Less relapse\n",
            "Immune system learns \n",
            "to terminate cancer \n",
            "cells upon return \n",
            "(immunomemory)\n",
            "It may not\n",
            "work\n",
            "Only patients with highly\n",
            "mutated tumours benefit. \n",
            "No response in low TMB \n",
            "tumours\n",
            "Immune Checkpoint Inhibitors (ICI)\n",
            "15\n",
            "Most patients (70%) do not respond to \n",
            "non-personalised immunotherapy.\n",
            "These patients require immunotherapy \n",
            "that specifically boost the immune cells \n",
            "against these tumour targets.\n",
            "Generic immunotherapy (e.g. checkpoint inhibitor therapy) has shown unprecedented results\n",
            "1st line advanced\n",
            "cancers\n",
            "Pembrolizumab\n",
            "(anti-PD1) is first \n",
            "line treatment in \n",
            "metastatic NSCLC\n",
            "4x more \n",
            "effective\n",
            "Can increase\n",
            "survival rates up to\n",
            "40%\n",
            "Fewer side \n",
            "effects\n",
            "No administration\n",
            "of toxic compounds\n",
            "Less relapse\n",
            "Immune system learns \n",
            "to terminate cancer \n",
            "cells upon return \n",
            "(immunomemory)\n",
            "It may not\n",
            "work\n",
            "Only patients with highly\n",
            "mutated tumours benefit. \n",
            "No response in low TMB \n",
            "tumours\n",
            "Immune Checkpoint Inhibitors (ICI)\n",
            "16\n",
            "Most patients do not respond to untargeted immunotherapy. \n",
            "These require a personalised approach.\n",
            "Personalised immunotherapy\n",
            "17\n",
            "Most patients do not respond to untargeted immunotherapy. \n",
            "These require a personalised approach.\n",
            "Personalised immunotherapy\n",
            "18\n",
            "Neoantigen prediction\n",
            "Most patients do not respond to untargeted immunotherapy. \n",
            "These require a personalised approach.\n",
            "Personalised immunotherapy\n",
            "19\n",
            "Neoantigen prediction\n",
            "Personalised\n",
            "vaccination\n",
            "Most patients do not respond to untargeted immunotherapy. \n",
            "These require a personalised approach.\n",
            "Personalised immunotherapy\n",
            "20\n",
            "21\n",
            "Identification of neoantigens is often crucial but challenging\n",
            "• Only a subset of tumour-specific genomic alterations are processed, presented, and \n",
            "immunogenic. Only this small fraction delivers clinical benefit in therapy \n",
            "• Therapy incorporating irrelevant targets will overload patient immune system and not \n",
            "cause tumour regression\n",
            "• Incorrect neoantigen identification can even lead to autotoxicity effects (irAEs)\n",
            "Personalised immunotherapy\n",
            "22\n",
            "How does myNEO apply \n",
            "NLP to cancer genomics?\n",
            "SNVs, indels\n",
            "Gene fusions\n",
            "Neoisoforms\n",
            "TEs\n",
            "lncRNAs\n",
            "WGS\n",
            "EXONS AND INTRONS\n",
            "100%\n",
            "ImmunoEngine identifies \n",
            "novel cancer-specific \n",
            "targets\n",
            "Rules out any possible \n",
            "interference from false \n",
            "epitopes\n",
            "Broadest possible \n",
            "tumour-specific variant & \n",
            "neoantigen detection\n",
            "Prioritization based on \n",
            "neoantigen presentation \n",
            "likelihood\n",
            "Neoantigen \n",
            "immunogenicity \n",
            "screening\n",
            "Triggers exceptionally \n",
            "strong immune \n",
            "responses\n",
            "Healthy\n",
            "DNA\n",
            "Tumour\n",
            "DNA & RNA\n",
            "OPTIMAL TARGETS\n",
            "Using unique computational biosimulation and AI heuristics with a validated predictive performance, myNEO’s\n",
            "ImmunoEngine rapidly cost-effectively identifies the set of neoantigens most likely to provide maximum clinical \n",
            "benefit and long-term resistance.\n",
            "myNEO proprietary computational platform: neoantigen discovery \n",
            "and selection at an unparalleled performance\n",
            "•\n",
            "Only neoantigens that are presented at the tumour cell \n",
            "surface have the potential to cause an immune \n",
            "response.\n",
            "•\n",
            "The prediction of neoantigen presentation is a crucial \n",
            "step of any discovery pipeline. \n",
            "•\n",
            "Recent increases in the amount of MHC ligandomic\n",
            "data available has opened the door to the development \n",
            "of a new generation of presentation prediction \n",
            "algorithms. \n",
            "24\n",
            "Immunotherapy epitope identification\n",
            "25\n",
            "Immunotherapy epitope identification\n",
            "ILHVQGAEAN\n",
            "TIRSPRVVDC \n",
            "QTHLSLDTSS \n",
            "SEGSTVDKLE  \n",
            "HGLNMLSEEA  \n",
            "DYPLKRSRGR  \n",
            "DRYEGELSQR  \n",
            "VNTDIYNLVF \n",
            "TIRSPRVVDC \n",
            "SPTKKADCAV  \n",
            "26\n",
            "Wells et al., 2020\n",
            "+\n",
            "-\n",
            "326,297 datapoints \n",
            "from public and \n",
            "private ligandomic\n",
            "mass spectrometry \n",
            "datasets\n",
            "datapoints generated \n",
            "in silico from the \n",
            "human proteome and \n",
            "HLAs from the \n",
            "positive dataset\n",
            "DATA COLLECTION\n",
            "+\n",
            "Each datapoint consists in 1 \n",
            "peptide + 1-6 HLA alleles\n",
            "DATA EMBEDDING\n",
            "Each HLA and peptide amino acid is \n",
            "transformed into a feature vector\n",
            "Sequence-to-sequence algorithm\n",
            "MODEL\n",
            "•\n",
            "BERT-like bidirectional encoder –\n",
            "transformer family\n",
            "•\n",
            "Leverages attention mechanisms to \n",
            "focus on most important input features\n",
            "neoMS: accurate MHCI presentation prediction\n",
            "27\n",
            "Mill, Nil Adell, et al. \"neoMS: Attention-based Prediction of MHC-I Epitope \n",
            "Presentation.\" bioRxiv (2022).\n",
            "•\n",
            "The neoMS model follows a BERT-like architecture, i.e.\n",
            "a bidirectional transformer architecture. \n",
            "•\n",
            "Input are peptide sequences and HLA sequences.\n",
            "•\n",
            "Input sequences are token-padded and one-hot \n",
            "encoded before processing by the model.\n",
            "•\n",
            "Positive data included MS identified peptides.\n",
            "•\n",
            "Negative data was synthetically generated.\n",
            "•\n",
            "Negative instances outnumbered positive instances by \n",
            "a 1:50 ratio.\n",
            "neoMS: accurate MHCI presentation prediction\n",
            "• Benchmarking: when confronted to \n",
            "the same (out-of-training) test \n",
            "dataset neoMS outperform current \n",
            "alternatives, affinity- or ligandomics-\n",
            "based, achieving a precision of \n",
            "0.61 at recall 0.4.\n",
            "• Extrapolation: the sequence-to-\n",
            "sequence structure of neoMS \n",
            "allows for accurate prediction even \n",
            "for HLAs lacking ligandomic data, \n",
            "thus minimising data collection \n",
            "costs.\n",
            "• Biological insights: neoMS \n",
            "predictions recapitulate HLA binding \n",
            "motifs and correlate with MS \n",
            "identification confidence scores\n",
            "Wells et al., 2020\n",
            "neoMS performance on HLA A*74:01, allele \n",
            "completely absent from training set\n",
            "neoMS performance compared to other \n",
            "prediction models\n",
            "neoMS-derived HLA-A*02:01 motif\n",
            "neoMS: accurate MHCI presentation prediction\n",
            "neoMS: accurate MHCI presentation prediction\n",
            "29\n",
            "neoMS: accurate MHCI presentation prediction\n",
            "Through more accurate presentation predictions neoMS can improve neoantigen-driven therapy \n",
            "efficacy.\n",
            "Better target selection decreases the number of ineffective epitopes (ie false positives) used in \n",
            "therapy.\n",
            "This can greatly reduce therapy development costs and minimize therapeutic development \n",
            "timelines, by limiting the number of neoantigen validation experiments needed.\n",
            "neoMS is an integral part of myNEOs neoantigen prediction pipeline. \n",
            "30\n",
            "Tumour-specific\n",
            "variant\n",
            "Filtering\n",
            "Peptide\n",
            "Tumour-specific\n",
            "surface peptide\n",
            "BETTER MHC PRESENTATION \n",
            "PREDICTION\n",
            "Immunogenicity\n",
            "responses\n",
            "BETTER IMMUNOGENICITY \n",
            "SCREENING\n",
            "EXTENSIVE TUMOUR \n",
            "ALTERATION SCREENING\n",
            "Neoantigen-driven immunotherapy\n",
            "Leader in neoantigen prediction and selection\n",
            "Expert in identification and validation of personalised immunotherapy\n",
            "www.myneo.me\n",
            "contact@myneo.me\n",
            "linkedin.com/company/myneo\n",
            "Ottergemsesteenweg 439 box 9, 9000 Ghent\n",
            "© 2022 myNEO N.V. All rights reserved\n",
            "myNEO\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import the drive module and mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Install PyMuPDF\n",
        "!pip install pymupdf\n",
        "\n",
        "# Step 3: Import fitz to work with PDFs\n",
        "import fitz\n",
        "\n",
        "# Step 4: Specify the path to the PDF file in Google Drive\n",
        "pdf_path = '/content/drive/My Drive/NLP_Summit_October2022/Lena Pfitzer.pdf'  # Adjust the path if your file is in a folder\n",
        "\n",
        "# Step 5: Open the PDF file and extract text from the first page\n",
        "doc = fitz.open(pdf_path)\n",
        "text = \"\"\n",
        "for page in doc:  # Iterate through each page\n",
        "    text += page.get_text()\n",
        "\n",
        "# Step 6: Print the extracted text\n",
        "print(text)\n",
        "\n",
        "# Step 7: Close the document\n",
        "doc.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import fitz  # Assuming PyMuPDF is used for PDF handling\n",
        "\n",
        "# Correct path to your PDF\n",
        "pdf_path = '/content/drive/My Drive/press-reports-2020-2024/OpenAI’s Next Big AI Effort, GPT-5, Is Behind Schedule and Crazy Expensive - WSJ.pdf'\n",
        "\n",
        "# Open the PDF file\n",
        "doc = fitz.open(pdf_path)\n",
        "text = \"\"\n",
        "for page in doc:\n",
        "    text += page.get_text()\n",
        "\n",
        "print(text)\n",
        "doc.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEs8mzcpWMH0",
        "outputId": "c7866782-f3ee-4ef5-f3bd-c524997fe684"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "This copy is for your personal, non-commercial use only. Distribution and use of this material are governed by our Subscriber Agreement and by copyright law. For\n",
            "non-personal use or to order multiple copies, please contact Dow Jones Reprints at 1-800-843-0008 or visit www.djreprints.com.\n",
            "https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693\n",
            "The Next Great Leap in AI Is Behind Schedule and Crazy Expensive\n",
            "OpenAI has run into problem after problem on its new artificial-intelligence project, code-named Orion\n",
            "By Deepa Seetharaman Follow\n",
            "Dec. 20, 2024 9:00 pm ET\n",
            "OpenAI CEO Sam Altman has predicted that GPT-5\n",
            "will represent a ‘significant leap forward.’ PHOTO:\n",
            "MIKE COPPOLA/GETTY IMAGES\n",
            "OpenAI’s new artificial-intelligence project is behind schedule and running up huge bills. It isn’t clear when—or if—it’ll work. There may not be enough data in the world to make it smart enough. \n",
            "The project, officially called GPT-5 and code-named Orion, has been in the works for more than 18 months and is intended to be a major advancement in the technology that powers ChatGPT. OpenAI’s closest partner and largest investor, Microsoft, had expected to see the\n",
            "new model around mid-2024, say people with knowledge of the matter.\n",
            "OpenAI has conducted at least two large training runs, each of which entails months of crunching huge amounts of data, with the goal of making Orion smarter. Each time, new problems arose and the software fell short of the results researchers were hoping for, people close\n",
            "to the project say.\n",
            "At best, they say, Orion performs better than OpenAI’s current offerings, but hasn’t advanced enough to justify the enormous cost of keeping the new model running. A six-month training run can cost around half a billion dollars in computing costs alone, based on public and\n",
            "private estimates of various aspects of the training. \n",
            "OpenAI and its brash chief executive, Sam Altman, sent shock waves through Silicon Valley with ChatGPT’s launch two years ago. AI promised to continually exhibit dramatic improvements and permeate nearly all aspects of our lives. Tech giants could spend $1 trillion on AI\n",
            "projects in the coming years, analysts predict. \n",
            "The weight of those expectations falls mostly on OpenAI, the company at ground zero of the AI boom.   \n",
            "The $157 billion valuation investors gave OpenAI in October is premised in large part on Altman’s prediction that GPT-5 will represent a “significant leap forward” in all kinds of subjects and tasks.\n",
            "GPT-5 is supposed to unlock new scientific discoveries as well as accomplish routine human tasks like booking appointments or flights. Researchers hope it will make fewer mistakes than today’s AI, or at least acknowledge\n",
            "doubt—something of a challenge for the current models, which can produce errors with apparent confidence, known as hallucinations. \n",
            "AI chatbots run on underlying technology known as a large language model, or LLM. Consumers, businesses and governments already rely on them for everything from writing computer code to spiffing up marketing copy and\n",
            "planning parties. OpenAI’s is called GPT-4, the fourth LLM the company has developed since its 2015 founding.\n",
            "While GPT-4 acted like a smart high-schooler, the eventual GPT-5 would effectively have a Ph.D. in some tasks, a former OpenAI executive said. Earlier this year, Altman told students in a talk at Stanford University that OpenAI\n",
            "could say with “a high degree of scientific certainty” that GPT-5 would be much smarter than the current model. \n",
            "There are no set criteria for determining when a model has become smart enough to be designated GPT-5. OpenAI can test its LLMs in areas like math and coding. It’s up to company executives to decide whether the model is\n",
            "smart enough to be called GPT-5 based in large part on gut feelings or, as many technologists say, “vibes.”\n",
            "So far, the vibes are off. \n",
            "OpenAI and Microsoft declined to comment for this article. In November, Altman said the startup wouldn’t release anything called GPT-5 in 2024. \n",
            "Training day\n",
            "From the moment GPT-4 came out in March 2023, OpenAI has been working on GPT-5.\n",
            "Longtime AI researchers say developing systems like LLMs is as much art as science. The most respected AI scientists in the world are celebrated for their intuition about how to get better results. \n",
            "Models are tested during training runs, a sustained period when the model can be fed trillions of word fragments known as tokens. A large training run can take several months in a data center with tens of thousands of expensive\n",
            "and coveted computer chips, typically from Nvidia.\n",
            "During a training run, researchers hunch over their computers for several weeks or even months, and try to feed much of the world’s knowledge into an AI system using some of the most expensive hardware in far-flung data\n",
            "centers.\n",
            "Altman has said training GPT-4 cost more than $100 million. Future AI models are expected to push past $1 billion. A failed training run is like a space rocket exploding in the sky shortly after launch.  \n",
            "Researchers try to minimize the odds of such a failure by conducting their experiments on a smaller scale—doing a trial run before the real thing.\n",
            "From the start, there were problems with plans for GPT-5. \n",
            "In mid-2023, OpenAI started a training run that doubled as a test for a proposed new design for Orion. But the process was sluggish, signaling that a larger training run would likely take an incredibly long time, which would in turn\n",
            "make it outrageously expensive. And the results of the project, dubbed Arrakis, indicated that creating GPT-5 wouldn’t go as smoothly as hoped.\n",
            "Number of parameters*, by GPT generation\n",
            "GPT-1 117 million\n",
            "GPT-2\n",
            "1.5 billion\n",
            "GPT-3\n",
            "175 billion\n",
            "OpenAI researchers decided to make some technical tweaks to strengthen Orion. They also concluded they needed more diverse, high-quality data. The public internet didn’t have enough, they felt. \n",
            "Jensen Huang, CEO of Nvidia, which makes the coveted and expensive chips needed to train AIs. PHOTO: JOSH EDELSON/AGENCE FRANCE-PRESSE/GETTY IMAGES\n",
            "Generally, AI models become more capable the more data they gobble up. For LLMs, that data is primarily from books, academic publications and other well-respected sources. This material helps LLMs express themselves more clearly and handle a wide range of tasks. \n",
            "For its prior models, OpenAI used data scraped from the internet: news articles, social-media posts and scientific papers. \n",
            "To make Orion smarter, OpenAI needs to make it larger. That means it needs even more data, but there isn’t enough. \n",
            "“It gets really expensive and it becomes hard to find more equivalently high-quality data,” said Ari Morcos, CEO of DatologyAI, a startup that builds tools to improve data selection. Morcos is building models with less—but much better—data, an approach he argues will\n",
            "make today’s AI systems more capable than the strategy embraced by all top AI firms like OpenAI.\n",
            "OpenAI’s solution was to create data from scratch. \n",
            "It is hiring people to write fresh software code or solve math problems for Orion to learn from. The workers, some of whom are software engineers and mathematicians, also share explanations for their work with Orion. \n",
            "Many researchers think code, the language of software, can help LLMs work through problems they haven’t already seen. \n",
            "OpenAI’s offices, where employees hunch over computers during AI training runs that can last weeks or months. PHOTO: CLARA MOKRI FOR THE WALL STREET JOURNAL\n",
            "Having people explain their thinking deepens the value of the newly created data. It’s more language for the LLM to absorb; it’s also a map for how the model might solve similar problems in the future.\n",
            "“We’re transferring human intelligence from human minds into machine minds,” said Jonathan Siddharth, CEO and co-founder of Turing, an AI-infrastructure company that works with OpenAI, Meta and others. \n",
            "In AI training, Turing executives said, a software engineer might be prompted to write a program that efficiently solves a complex logic problem. A mathematician might have to calculate the maximum height of a pyramid constructed out of one million basketballs. The\n",
            "answers—and, more important, how to reach them—are then incorporated into the AI training materials.\n",
            "OpenAI has worked with experts in subjects like theoretical physics, to explain how they would approach some of the toughest problems in their field. This can also help Orion get smarter.  \n",
            "The process is painfully slow. GPT-4 was trained on an estimated 13 trillion tokens. A thousand people writing 5,000 words a day would take months to produce a billion tokens. \n",
            "OpenAI also started developing what is called synthetic data, or data created by AI, to help train Orion. The feedback loop of AI creating data for AI can often cause malfunctions or result in nonsensical answers, research has shown. \n",
            "Scientists at OpenAI think they can avoid those problems by using data generated by another of its AI models, called o1, people familiar with the matter said. \n",
            "OpenAI’s already-difficult task has been complicated by internal turmoil and near-constant attempts by rivals to poach its top researchers, sometimes by offering them millions of dollars. \n",
            "Last year, Altman was abruptly fired by OpenAI’s board of directors, and some researchers wondered if the company would continue. Altman was quickly reinstated as CEO and set out to overhaul OpenAI’s governance structure. \n",
            "GPT-4 1.76 trillion†\n",
            "*Settings that determine how an AI processes\n",
            "information and makes decisions †Estimate\n",
            "Source: OpenAI (GPT-1, -2, -3); SemiAnalysis (GPT-4)\n",
            "More than two dozen key executives, researchers and longtime employees have left OpenAI this year, including co-founder and Chief Scientist Ilya Sutskever and Chief Technology Officer Mira Murati. This past Thursday, Alec Radford, a widely admired researcher who\n",
            "served as lead author on several of OpenAI’s scientific papers, announced his departure after about eight years at the company. \n",
            "Reboot\n",
            "By early 2024, executives were starting to feel the pressure. GPT-4 was already a year old and rivals were starting to catch up. A new LLM from Anthropic was rated by many in the industry as better than GPT-4. Several months later, Google launched the most viral new AI\n",
            "application of the year, called NotebookLM.\n",
            "As Orion stalled, OpenAI started developing other projects and applications. They included slimmed-down versions of GPT-4 and Sora, a product that can produce AI-generated video. \n",
            "Google is among the competitors vying with OpenAI for dominance in artificial intelligence. PHOTO: JUSTIN SULLIVAN/GETTY IMAGES\n",
            "That led to fighting over limited computing resources between teams working on new products and Orion researchers, according to people familiar with the matter. \n",
            "Competition among AI labs has grown so fierce that major tech companies publish fewer papers about recent findings or breakthroughs than is typical in science. As money flooded the market two years ago, tech companies started viewing the results of this research as\n",
            "trade secrets that needed guarding. Some researchers take this so seriously they won’t work on planes, coffee shops or anyplace where someone could peer over their shoulder and catch a glimpse of their work. \n",
            "That secretive attitude has frustrated many longtime AI researchers, including Yann LeCun, chief AI scientist at Meta. LeCun said work from OpenAI and Anthropic should no longer be viewed as research, but as “advanced product development.” \n",
            "“If you’re doing it on a commercial clock, it’s not called research,” said LeCun on the sidelines of a recent AI conference, where OpenAI had a minimal presence. “If you’re doing it in secret, it’s not called research.” \n",
            "In early 2024, OpenAI prepared to give Orion another try, this time armed with better data. Researchers launched a couple of smaller-scale training runs over the first few months of the year to build up confidence.\n",
            "By May, OpenAI’s researchers decided they were ready to attempt another large-scale training run for Orion, which they expected to last through November. \n",
            "Once the training began, researchers discovered a problem in the data: It wasn’t as diversified as they had thought, potentially limiting how much Orion would learn. \n",
            "The problem hadn’t been visible in smaller-scale efforts and only became apparent after the large training run had already started. OpenAI had spent too much time and money to start over.\n",
            "Instead, researchers scrambled to find a wider range of data to feed the model during the training process. It isn’t clear if this strategy proved fruitful.\n",
            "Orion’s problems signaled to some at OpenAI that the more-is-more strategy, which had driven much of its earlier success, was running out of steam.\n",
            "OpenAI isn’t the only company worrying that progress has hit a wall. Across the industry, a debate is raging over whether improvement in AIs is starting to plateau.\n",
            "Ilya Sutskever stepped down this year as OpenAI’s chief scientist. PHOTO: AMIR COHEN/REUTERS\n",
            "Sutskever, who recently co-founded a new AI firm called Safe Superintelligence or SSI, declared at a recent AI conference that the age of maximum data is over. “Data is not growing because we have but one internet,” he told a crowd of researchers, policy experts and\n",
            "scientists. “You can even go as far as to say that data is the fossil fuel of AI.” \n",
            "And that fuel was starting to run out.\n",
            "Appeared in the December 21, 2024, print edition as 'The Next Great Leap in AI Is Behind Schedule and Crazy Expensive'.\n",
            "Videos\n",
            "Reasoning\n",
            "Their struggles on Orion led OpenAI researchers to a new approach to making an LLM smarter: reasoning. Spending a long time “thinking” could allow LLMs to solve difficult problems they haven’t been trained on, researchers say.\n",
            "Behind the scenes, OpenAI’s o1 offers several responses to each question and analyzes them to find the best one. It can perform more complex tasks, like writing a business plan or creating a crossword puzzle, while explaining its reasoning—which helps the model learn a\n",
            "little bit from each answer. \n",
            "Researchers at Apple recently released a paper that argues reasoning models, including versions of o1, were most likely mimicking the data they saw in training rather than actually solving new problems. \n",
            "The Apple researchers said they found “catastrophic performance drops” if questions were changed to include irrelevant details—like tweaking a math problem about kiwis to note that some of the fruits were smaller than others.\n",
            "In September, OpenAI launched a preview of its o1 reasoning model and released the full version of o1 earlier this month. \n",
            "All that added brainpower is expensive. OpenAI is now paying to generate multiple answers to a single query, instead of just one. \n",
            "In a recent TED talk, one of OpenAI’s senior research scientists played up the advantages of reasoning. \n",
            "“It turned out that having the bot think for just 20 seconds in a hand of poker got the same boost in performance as scaling up the model by 100,000x and training for 100,000 times longer,” said Noam Brown, the OpenAI scientist. \n",
            "A more advanced and efficient reasoning model could form the underpinnings of Orion. OpenAI researchers are pursuing that approach and hoping to combine it with the old method of more data, some of which could come from OpenAI’s other AI models. OpenAI could then\n",
            "refine the results with material generated by people. \n",
            "On Friday, Altman announced plans for a new reasoning model smarter than anything the company has released before. He didn’t say anything about when, or whether, a model worthy of being called GPT-5 is coming.\n",
            "—Tom Dotan contributed to this article.\n",
            "Write to Deepa Seetharaman at deepa.seetharaman@wsj.com\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "\n",
        "# Assuming the file is in the root of 'My Drive'\n",
        "#folder_path = '/content/drive/My Drive/'\n",
        "#files = os.listdir(folder_path)\n",
        "#print(files)\n"
      ],
      "metadata": {
        "id": "GRvzS0bqQk-c"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}